### Introduction

The Multi-Domain Fraud Detection System is an innovative platform developed to address the growing challenges of fraud detection in the digital era.
The application focuses on three key domains:
1. GPT-Generated Text Detection
2. Credit Card Fraud Detection
3. Automobile Fraud Detection

The application leverages advanced machine learning models, user-friendly UI components, and robust authentication mechanisms to ensure
accuracy, security, and user engagement.

---

### Key Components and Features

#### 1. User Interface
The Streamlit framework serves as the foundation for the user interface. Key features include:
- A visually appealing sidebar with registration and login functionalities.
- Integration of user authentication through OTP sent via Telegram.
- Dynamic pages for fraud detection across multiple domains.
- Use of HTML and CSS for enhanced styling and user experience.

#### 2. User Authentication System
The application incorporates a robust authentication system that uses OTP (One-Time Password) verification via Telegram.
Key functionalities include:
- User registration with unique username and email validation.
- Database management using SQLite to store user credentials.
- OTP generation, delivery through Telegram API, and validation within a set timeout.
- Timer functionality to ensure OTP expiration after 30 seconds.

#### 3. Fraud Detection Modules
Each module is designed to cater to a specific type of fraud:
- **GPT-Generated Text Detection**: Utilizes perplexity-based analysis to identify AI-generated content. The system analyzes text patterns
to predict whether the input text is human-written or generated by AI.
- **Credit Card Fraud Detection**: Employs an Artificial Neural Network (ANN) trained on transaction datasets to classify transactions
as fraudulent or normal. Key features analyzed include transaction amount, location, and time.
- **Automobile Fraud Detection**: Uses supervised learning models to detect fraudulent insurance claims. Features such as vehicle category,
accident area, and claim history are analyzed to predict anomalies.

#### 4. Database Management
SQLite is used to store and manage user credentials. A database file `users.db` is initialized and structured to hold information such as username,
name, email, and location. SQL commands are used for operations like registration, validation, and fetching user details.

#### 5. Telegram Integration for OTP Delivery
The Telegram Bot API is used to deliver OTPs to users. Key elements include:
- Integration of Telegram bot credentials via environment variables.
- A dedicated function to generate OTP and send it to the user's Telegram account.
- Error handling for message delivery failures.

#### 6. Session State Management
Streamlit's session state is utilized to manage variables such as:
- Randomly generated OTPs.
- Timer status and expiration flags.
- Login status and user details.

---

### Application Workflow

#### User Registration
1. Users can create an account by providing their name, email, username, and location.
2. The application validates the inputs and ensures no duplicate entries exist in the database.
3. Successful registrations are stored in the SQLite database.

#### User Login with OTP
1. Users enter their username and request an OTP.
2. The application verifies the username in the database.
3. An OTP is generated and sent to the user's Telegram account.
4. Users enter the OTP within 30 seconds to authenticate.
5. Successful authentication logs the user into the system.

#### Fraud Detection Modules
1. Once authenticated, users can access the fraud detection functionalities.
2. The system dynamically loads the module based on user selection.
3. For each domain:
   - Inputs are collected from the user.
   - Data preprocessing and feature extraction are performed.
   - The appropriate model is invoked for prediction.
   - Results are displayed with actionable insights.

---

### Technologies and Libraries Used

1. **Streamlit**: For building the web-based user interface.
2. **SQLite**: For database management and user credential storage.
3. **Telegram Bot API**: For OTP delivery and user communication.
4. **Python Libraries**:
   - `pandas`, `numpy`: For data handling and preprocessing.
   - `random`: For generating OTPs.
   - `time`: For managing timers and OTP expiration.
   - `os`: For managing environment variables.
   - `base64`, `requests`: For handling API interactions and encoding.

---

### Code Modularity and Scalability

The code is modularly designed with separate classes and functions for:
- User authentication (`UserAuthApp`).
- Fraud detection functionality (`FraudDetectionApp`).
This modular approach ensures scalability, allowing for easy addition of new fraud detection domains or features.

---

### Security Considerations

1. **Data Privacy**: User credentials are securely stored in the SQLite database with unique constraints to prevent duplication.
2. **Authentication Security**: OTP-based authentication ensures secure access, with expiration mechanisms to mitigate misuse.
3. **Telegram Integration**: Communication via Telegram ensures secure and reliable delivery of OTPs.

---

### Future Enhancements

1. **GPT-Generated Text Detection**:
   - Expand detection capabilities to include AI-generated images and videos.
   - Incorporate advanced natural language processing techniques for higher accuracy.

2. **Credit Card Fraud Detection**:
   - Real-time transaction monitoring and fraud alerting.
   - Support for multi-currency and cross-border transactions.

3. **Automobile Fraud Detection**:
   - Integration with IoT data for real-time telematics analysis.
   - Enhanced policy recommendations for insurers.

4. **User Management**:
   - Role-based access control for admin and user functionalities.
   - Enhanced reporting and analytics for fraud cases.



Overview of User Authentication

Core Components and Functionality
User Interface (UI):

Built using Streamlit to provide an interactive and visually appealing experience.
Features include:
User registration with input fields for first name, last name, email, username, and location.
OTP-based login with real-time validation and feedback.
Dynamic background styling using images for the main app and sidebar.
Database Management:

SQLite database (users.db) for storing user credentials and information.
Table schema includes:
username: Unique identifier for users.
email: Unique email for each user.
name, last_name, and location: Additional user information.
Authentication Workflow:

Registration:
Users register by providing their details.
Input validation ensures unique usernames and emails.
Registration status is displayed as a success or error message.
Login with OTP:
Users provide their username to request an OTP.
Username validation ensures legitimate users can request OTPs.
OTP is generated and sent to the user's Telegram account.
Users enter the OTP for authentication within a 30-second window.
OTP Generation and Telegram Integration:

A random 4-digit OTP is generated using Pythonâ€™s random library.
OTPs are sent to users via Telegram using the Telegram Bot API.
Real-time feedback informs users about OTP delivery status.
Session Management:

Streamlit's st.session_state is used to manage session-specific variables, such as:
OTP (random_number)
Timer start time (start_time)
Timer running state (timer_running)
OTP expiration flag (is_code_expired)
User login state (logged_in) and username.
Background and Sidebar Customization:

Dynamic background and sidebar images enhance the app's visual appeal.
Base64 encoding is used to embed images directly into the Streamlit app.
Application Workflow
User Registration:

User enters their details in the registration form.
Validation ensures unique username and email.
Upon successful registration, user information is saved in the SQLite database.
OTP Login:

User enters their username to request an OTP.
The system verifies the username in the database.
A 4-digit OTP is generated and sent to the user's Telegram account.
User inputs the OTP for verification within a 30-second window.
Successful verification logs the user in.
Timer Management:

The OTP is valid for 30 seconds.
A real-time countdown is displayed to inform users of the remaining time.
OTPs expire after 30 seconds, requiring users to request a new one.
Logout Functionality:

Users can log out to clear their session and return to the main screen.
Technologies and Libraries Used
Streamlit:

For creating the web application interface.
Provides widgets for user input and dynamic updates.
SQLite:

Lightweight, file-based database for storing user credentials.
Telegram Bot API:

For sending OTPs securely to users via Telegram.
Python Libraries:

random: To generate random OTPs.
time: For timer functionality and managing OTP expiration.
base64: To encode images for use in the Streamlit app.
requests: For sending HTTP requests to the Telegram API.
Security Features
Data Security:

User credentials are stored securely in the SQLite database with unique constraints.
OTP Security:

OTPs are time-sensitive and expire after 30 seconds to prevent misuse.
Telegram integration ensures secure delivery of OTPs.
Validation:

Registration ensures unique usernames and emails.
Login verifies the username before sending an OTP.
Future Enhancements
Improved User Experience:

Add email-based OTP delivery as an alternative to Telegram.
Include a password-based authentication option.
Enhanced Security:

Integrate hashed storage for sensitive data like email and OTPs.
Implement CAPTCHA to prevent automated registrations.
Scalability:

Extend the database schema to include user roles and additional details.
Add support for multiple authentication methods.
Real-Time Analytics:

Provide admin dashboards for monitoring login activities and user registrations.


Technical Description of the Fraud Detection Application
This application, built using Streamlit, provides a multi-domain fraud detection platform leveraging machine learning models and user-friendly interfaces. It enables users to identify fraud in credit card transactions, insurance claims, and AI-generated text detection through perplexity analysis.

Core Components and Functionality
1. User Interface (UI)
The UI is designed to enhance user engagement and provide a smooth experience:

Streamlit-based Sidebar: Displays a navigation menu and additional information for each fraud detection model.
Dynamic Model Selection: Users can choose among three fraud detection types:
Credit Card Fraud Detection
Insurance Fraud Detection
AI-Generated Text Detection (GPT Detection)
2. Machine Learning Model Integration
The application integrates multiple models for different fraud detection tasks:

Credit Card Fraud Detection:
Uses an Artificial Neural Network (ANN) model (ann_model.pkl).
The model predicts fraudulent transactions based on user-uploaded transaction datasets.
Insurance Fraud Detection:
Employs a supervised learning model (fraud_detection_model.pkl) with label encoding for categorical features.
Predictions are made based on user-provided inputs regarding insurance claims.
AI-Generated Text Detection:
Utilizes a perplexity-based model (PerplexityApp) to analyze and detect machine-generated text.
3. Credit Card Fraud Detection Module
Input: Users upload a CSV file containing transaction data.
Processing:
Data is previewed to ensure compatibility.
The ANN model predicts labels for each transaction.
Output:
Predicted labels (Fraud Detected or Normal) are displayed in a dataframe.
Users can download predictions as a CSV file.
4. Insurance Fraud Detection Module
Input:
Users select details related to insurance claims, such as:
Policy type
Vehicle category
Accident area
Age of the policyholder
Processing:
Label encoders preprocess categorical inputs.
The model predicts the likelihood of fraud based on input features.
Output:
Fraud predictions are displayed with a color-coded message: red for fraud and green for no fraud.
5. GPT-Generated Text Detection
This module (provided by PerplexityApp) enables users to paste or upload text to analyze its perplexity.
The application evaluates whether the text was generated by AI.
Application Workflow
1. Model Selection
Users select the desired fraud detection type from the sidebar menu.
The application dynamically adjusts the interface and information for the chosen model.
2. Credit Card Fraud Detection
Users upload a CSV file.
The ANN model processes the data and classifies transactions.
Predictions are displayed in a tabular format.
Users can download the results.
3. Insurance Fraud Detection
Users input claim details through dropdown menus and text fields.
The model processes the inputs using label encoding.
A binary prediction (fraud or no fraud) is displayed in a visually distinct format.
4. AI-Generated Text Detection
Users provide text data via file upload or text input.
The perplexity analysis determines if the content is AI-generated.
Results are displayed along with relevant analysis.
Technologies and Libraries Used
Streamlit:

Framework for building interactive web applications.
Provides widgets for user input and real-time interaction.
Pandas:

Handles data preprocessing and visualization.
Manages uploaded CSV files.
NumPy:

Facilitates numerical computations and feature preprocessing.
Pickle and Joblib:

Used for serializing and loading trained machine learning models.
Machine Learning Models:

Artificial Neural Network (ANN) for credit card fraud detection.
Supervised learning models with categorical feature encoding for insurance fraud detection.
Security and Performance Considerations
Data Privacy:

Uploaded files are not stored permanently, ensuring user privacy.
Sensitive model files are loaded dynamically, minimizing security risks.
Performance Optimization:

Models and encoders are loaded only when needed, optimizing resource usage.
Streamlit's caching mechanism ensures efficient data handling.
User Feedback:

Intuitive design with clear feedback (e.g., fraud detected in red text) enhances usability.
Future Enhancements
Enhanced Detection Models:

Upgrade the ANN model for real-time fraud detection.
Integrate additional features, such as transaction geolocation data, for credit card fraud detection.
Scalability:

Extend the application to support large-scale datasets.
Deploy on cloud platforms for wider accessibility.
Improved AI Detection:

Incorporate advanced natural language processing (NLP) techniques to enhance AI-generated text detection accuracy.





Technical Description of the Perplexity Detection Application
This application is designed to calculate the perplexity of a given text to determine whether it is likely human-written or generated by GPT models. Perplexity is a statistical measure used in natural language processing to gauge the quality or predictability of a sequence of text. Lower perplexity values typically indicate text generated by a language model like GPT, while higher perplexity suggests human-written content.

Core Components and Functionality

1. Model and Tokenizer Initialization

GPT-2 Model: A pre-trained GPT-2 language model is used for computing perplexity.
GPT2LMHeadModel: Handles language modeling tasks, including next-word prediction and perplexity calculations.
Tokenizer: GPT-2's tokenizer is used to preprocess the input text, converting it into tokenized format suitable for the model.

2. Perplexity Calculation
The get_perplexity function calculates perplexity as follows:
Tokenization: The input text is tokenized into numerical format using the GPT-2 tokenizer.
Model Forward Pass: The tokenized inputs are passed to the GPT-2 model, generating output logits and a loss value.
Loss Extraction: The loss value represents the negative log-likelihood of the input text.
Perplexity Computation: Exponentiation of the loss value gives the perplexity score:
Perplexity
=ð‘’loss
Perplexity=e
loss

3. User Interface (UI)

Text Input:
A multi-line text area (st.text_area) allows users to input the text they want to analyze.
A default placeholder guides users on what to enter.
Button Interaction:
A button (st.button) triggers the perplexity calculation when clicked.
Result Display:
The perplexity score is displayed with interpretive feedback:
High Perplexity (>50): Likely human-generated text.
Low Perplexity (â‰¤50): Likely GPT-generated text.

4. Streamlit Integration

The application uses Streamlit to provide an interactive and visually appealing interface:
Page title and description guide the user.
Text input and output are dynamically updated based on user interaction.
Feedback messages (e.g., "Likely human-generated text") enhance usability.
Application Workflow
Input Text:

The user enters text into the provided text area.
Placeholder text ("Type your text here.") ensures the input field is intuitive.
Perplexity Calculation:

The user clicks the "Calculate Perplexity" button.
The get_perplexity function processes the input text:
Tokenizes the text.
Computes the perplexity score using GPT-2.
Result Display:

The application dynamically displays the perplexity score.
Contextual feedback informs the user whether the text is likely human-generated or GPT-generated.
Error Handling:

If the text area is empty, the application prompts the user to enter text.
Technologies and Libraries Used
Streamlit:

Provides a user-friendly web interface for the application.
Widgets include text_area for text input and button for interaction.
Hugging Face Transformers:

GPT2LMHeadModel: Pre-trained GPT-2 model for language modeling tasks.
GPT2Tokenizer: Tokenizes text inputs for the GPT-2 model.
PyTorch:

Handles tensor computations for model inputs and outputs.
torch.exp: Used for exponentiating the loss to compute perplexity.
Security and Performance Considerations
Model Efficiency:

GPT-2 is a computationally intensive model. The application is best suited for moderate-length text to ensure responsiveness.
Input Validation:

The application checks for empty inputs and prompts the user to enter valid text.
Resource Utilization:

Avoids redundant model loads by initializing the model and tokenizer once.
Future Enhancements
Extended Detection:

Include models for detecting other types of AI-generated content (e.g., images, audio).
Multi-Model Support:

Provide options to compare perplexity across multiple language models.
Performance Optimization:

Implement caching mechanisms to reduce computation time for repeated queries.


Technical Description of the Fraud Detection Model Training Script
This script is designed to train a machine learning model to detect fraudulent activities using a Random Forest Classifier. The script also preprocesses the data by encoding categorical variables and splits the dataset into training and testing sets before training the model. The trained model and the encoders are saved for future use.

Core Components and Functionality
1. Dataset Loading
File: The dataset is loaded from a CSV file (Dataset/fraud_oracle_balanced_top10_original.csv) using pandas.
Structure:
The dataset includes categorical variables and a target variable (FraudFound_P) indicating fraud detection outcomes.
2. Data Preprocessing
Categorical Encoding:

Categorical variables are encoded using LabelEncoder from sklearn.preprocessing.
Each categorical column is transformed into numerical format to make the data compatible with the Random Forest Classifier.
Encoders are stored in a dictionary for reuse.
Categorical Columns:

BasePolicy, Fault, VehicleCategory, VehiclePrice, PolicyType, Sex, AddressChange_Claim, AccidentArea, AgeOfPolicyHolder.
3. Feature and Target Separation
The dataset is split into:
Features (X): All columns except the target variable (FraudFound_P).
Target (y): The column FraudFound_P.
4. Train-Test Split
The dataset is split into training and testing sets using train_test_split from sklearn.model_selection:
80% of the data is used for training.
20% is reserved for testing.
A random_state of 42 ensures reproducibility.
5. Model Training
Algorithm: Random Forest Classifier
Chosen for its robustness and ability to handle categorical data and imbalanced datasets.
The random_state parameter ensures consistent results across runs.
6. Model and Encoder Saving
The trained model and label encoders are saved using pickle for future use:
Model File: Model/fraud_detection_model.pkl.
Encoders File: Model/label_encoders.pkl.
Workflow
Data Loading:

Load the dataset using pandas.
Data Preprocessing:

Encode categorical variables using LabelEncoder.
Store encoders in a dictionary for later use.
Feature-Target Separation:

Separate the features (X) and target (y).
Dataset Splitting:

Split the dataset into training and testing sets.
Model Training:

Train a RandomForestClassifier on the training set.
Model Saving:

Save the trained model and label encoders using pickle.
Output:

A confirmation message is printed to indicate successful saving of the model and encoders.
Technologies and Libraries Used
Pandas:

For data loading and preprocessing.
Facilitates easy manipulation and transformation of the dataset.
Scikit-learn:

LabelEncoder: Encodes categorical variables into numerical format.
train_test_split: Splits the dataset into training and testing sets.
RandomForestClassifier: Machine learning algorithm for classification.
Pickle:

Serializes the trained model and encoders for later use in prediction scripts.
Advantages of Using Random Forest
Robustness:

Handles both numerical and categorical data effectively.
Resistant to overfitting due to ensemble learning.
Feature Importance:

Provides insights into which features are most significant for fraud detection.
Scalability:

Can handle large datasets efficiently.
Future Enhancements
Hyperparameter Tuning:

Use grid search or random search to optimize the Random Forest hyperparameters for better performance.
Evaluation Metrics:

Include metrics like precision, recall, and F1-score to assess model performance.
Additional Features:

Integrate additional features to improve the model's predictive power.
Cross-Validation:

Implement cross-validation to ensure model generalizability.